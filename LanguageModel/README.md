To produce the best result. You can run Distilled-GPT2 by 

```bash
python distilgpt2.py
```
If you can't connect to HuggingFace, you can download the model locally, and `from_pretrained('path/to/your/model')`
Before that you need to modify `outputpath` in `configs/dinov2b14_attention.yaml` and

Download the pt files in this [link](https://jbox.sjtu.edu.cn/l/31i0MS) and put them under `experiments/dinov2b14_attention_sigmoid_15` directory.
